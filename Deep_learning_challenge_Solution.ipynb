{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data...\n"
     ]
    }
   ],
   "source": [
    "# get mnist\n",
    "print(\"getting data...\")\n",
    "x_mnist, y_mnist = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "x_mnist = x_mnist.reshape(-1, 28, 28)\n",
    "y_mnist = y_mnist.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_mnist.shape)\n",
    "print(y_mnist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mnist_train, x_mnist_test, y_mnist_train, y_mnist_test = train_test_split(x_mnist, y_mnist, train_size=.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 28, 28)\n",
      "(14000, 28, 28)\n",
      "(56000,)\n",
      "(14000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_mnist_train.shape)\n",
    "print(x_mnist_test.shape)\n",
    "print(y_mnist_train.shape)\n",
    "print(y_mnist_test.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize an image from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cc02686a88>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "i = 111\n",
    "print(y_mnist_train[i]) \n",
    "plt.imshow(x_mnist_train[i], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the images of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in x_train 56000\n",
      "Number of images in x_test 14000\n"
     ]
    }
   ],
   "source": [
    "x_train = x_mnist_train.reshape(x_mnist_train.shape[0], 28, 28, 1)\n",
    "x_test = x_mnist_test.reshape(x_mnist_test.shape[0], 28, 28, 1)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model - Basic conv network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1750/1750 [==============================] - 15s 9ms/step - loss: 0.2224 - accuracy: 0.9320\n",
      "Epoch 2/15\n",
      "1750/1750 [==============================] - 15s 9ms/step - loss: 0.0873 - accuracy: 0.9728\n",
      "Epoch 3/15\n",
      "1750/1750 [==============================] - 15s 8ms/step - loss: 0.0605 - accuracy: 0.9810\n",
      "Epoch 4/15\n",
      "1750/1750 [==============================] - 15s 8ms/step - loss: 0.0456 - accuracy: 0.9855\n",
      "Epoch 5/15\n",
      "1750/1750 [==============================] - 14s 8ms/step - loss: 0.0360 - accuracy: 0.9881\n",
      "Epoch 6/15\n",
      "1750/1750 [==============================] - 14s 8ms/step - loss: 0.0302 - accuracy: 0.9898\n",
      "Epoch 7/15\n",
      "1750/1750 [==============================] - 15s 8ms/step - loss: 0.0253 - accuracy: 0.9912\n",
      "Epoch 8/15\n",
      "1750/1750 [==============================] - 15s 8ms/step - loss: 0.0200 - accuracy: 0.9927\n",
      "Epoch 9/15\n",
      "1750/1750 [==============================] - 15s 8ms/step - loss: 0.0208 - accuracy: 0.9931\n",
      "Epoch 10/15\n",
      "1750/1750 [==============================] - 15s 9ms/step - loss: 0.0177 - accuracy: 0.9941\n",
      "Epoch 11/15\n",
      "1750/1750 [==============================] - 15s 9ms/step - loss: 0.0155 - accuracy: 0.9947\n",
      "Epoch 12/15\n",
      "1750/1750 [==============================] - 15s 8ms/step - loss: 0.0157 - accuracy: 0.9948\n",
      "Epoch 13/15\n",
      "1750/1750 [==============================] - 15s 9ms/step - loss: 0.0141 - accuracy: 0.9951\n",
      "Epoch 14/15\n",
      "1750/1750 [==============================] - 15s 9ms/step - loss: 0.0137 - accuracy: 0.9951\n",
      "Epoch 15/15\n",
      "1750/1750 [==============================] - 16s 9ms/step - loss: 0.0128 - accuracy: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc025d2748>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_mnist_train, epochs=15)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 1s 3ms/step - loss: 0.0780 - accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07796843349933624, 0.985785722732544]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once model is working, we write a function that does the following\n",
    "\n",
    "#### 1) Accepts images from the test set (Size - 14000 x 1)\n",
    "#### 2) Makes predictions on the test set (Size - 14000 x 10) . Here output is one-hot encoded\n",
    "#### 3) We take argmax to get the label with the highest probablity . (Size - 14000 x 1)\n",
    "#### 4) Convert it into label-pairs (Size- 7000 x 2)\n",
    "#### 5) Take the sum of digits in the label_pairs array (Size - 7000 x 1)\n",
    "\n",
    "### Hence, we have written a function that accepts the images from the dataset, makes predictions, generates the image-pairs and returns the sum of digits in the individual pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sum(x):\n",
    "    \n",
    "    print('The shape of images -' ,x.shape) \n",
    "    pred = model.predict(x)\n",
    "    print('The shape of the predictions is -' , pred.shape)\n",
    "    label = pred.argmax(axis=1) \n",
    "    print('The shape of the labels is -' , label.shape)\n",
    "    \n",
    "    ## We generate label-pairs \n",
    "    l_pairs = label.reshape(label.shape[0]//2, 2)\n",
    "    print('The shape of the label pairs generated -', l_pairs.shape)\n",
    "    l_sums =  np.sum(l_pairs, axis=1)\n",
    "    print('The sum of the individual image label-pairs' , l_sums)\n",
    "    \n",
    "    return l_sums\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of images - (14000, 28, 28, 1)\n",
      "The shape of the predictions is - (14000, 10)\n",
      "The shape of the labels is - (14000,)\n",
      "The shape of the label pairs generated - (7000, 2)\n",
      "The sum of the individual image label-pairs [ 4  3 16 ...  3  9 10]\n"
     ]
    }
   ],
   "source": [
    "l_sums = compute_sum(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also write a function that directly takes images from the dataset and returns their sum.\n",
    "\n",
    "### This is the ground-truth i.e. Actual image-pairs and their corresponding sums in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sums(x, y):\n",
    "    x_pairs = x.reshape(x.shape[0]//2, 2, *x.shape[-2:])\n",
    "    y_pairs = y.reshape(y.shape[0]//2, 2)\n",
    "    y_sums = np.sum(y_pairs, axis=1)\n",
    "    \n",
    "    print('The shape of the x_pairs array' , x_pairs.shape)\n",
    "    print('The shape of the array of labels' , y_sums.shape) \n",
    "    print('The sum of pairs of digits in the x_pairs array is :' , y_sums)\n",
    "\n",
    "    return x_pairs, y_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the x_pairs array (7000, 2, 28, 28)\n",
      "The shape of the array of labels (7000,)\n",
      "The sum of pairs of digits in the x_pairs array is : [ 4  3 16 ...  3  9 10]\n"
     ]
    }
   ],
   "source": [
    "_, y_sums = convert_to_sums(x_mnist_test, y_mnist_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To determine Accuracy of the model -\n",
    "\n",
    "### - Compare the Actual sum obtained from the ACTUAL digit-pairs present in the dataset with the Sum obtained from the PREDICTED digits in the dataset\n",
    "\n",
    "### That is we compare l_sums with y_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model- 97.2\n"
     ]
    }
   ],
   "source": [
    "acc = 100*((l_sums == y_sums).sum()/l_sums.shape[0]) \n",
    "print('Accuracy of the model-' , acc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "### There can be multiple ways to solve this problem but I came up with this method!\n",
    "\n",
    "#### I wrote a function that takes a pairs of images or any image from the dataset, predicts the labels, generates the label-pairs and then the corresponding sums of the pairs of digits.\n",
    "\n",
    "#### I also wrote a function to directly take images from test set, generate the actual image-pairs and returns the corresponding sums of the pairs of digits.\n",
    "\n",
    "#### Evaluation Accuracy of the CNN model - 98.57 %\n",
    "\n",
    "#### Accuracy of computing sums ( When compared with the actual sums or the ground-truth) - 97.2 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
